{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: sanketvmehta\n",
    "# Base code from: http://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduction to Multi-layer LSTM in Pytorch\n",
    "======================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![LSTM Architecture](lstm_architecture.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = nn.LSTM(input_size=6, hidden_size=4, num_layers=2, bias=True, \n",
    "               batch_first=False, dropout=0, bidirectional=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Args:\n",
    "- **input_size:** The number of expected features in the input $x_t$ (_6 in our case_)    \n",
    "- **hidden_size:** The number of features in the hidden state $h_t$ (_4 in our case_)    \n",
    "- **num_layers:** Number of recurrent layers (_2 in our case_)    \n",
    "- **bias:** If _False_, then the layer does not use bias weights $b_{ih}$ and $b_{hh}$ Default: _True_    \n",
    "- **batch_first:** If _True_, then the input and output tensors are provided as (_batch, seq, feature_)    \n",
    "- **dropout:** If non-zero, introduces a dropout layer on the outputs of each RNN layer except the last layer    \n",
    "- **bidirectional:** If _True_, becomes a bidirectional RNN. Default: _False_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the hidden state "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "h0 = autograd.Variable(torch.randn(2, 1, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**h0 (num_layers * num_directions, batch, hidden_size):** tensor containing the initial hidden state for each element in the batch. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the cell state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "c0 = autograd.Variable(torch.randn(2, 1, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c0 (num_layers * num_directions, batch, hidden_size):** tensor containing the initial cell state for each element in the batch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the input for the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = autograd.Variable(torch.randn(5, 1, 6)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**input (seq_len, batch, input_size):** tensor containing the features of the input sequence where first\n",
    "argument corresponds to length of the sequence (_5 in our case_), second argument corresponds to size of the batch (_1 in our case_) and third argument corresponds to the number of features in the input (_6 in our case_)\n",
    "\n",
    "**NOTE:** We have above ordering because \"batch_first\" is set to 'False' in our network definition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the output from Multi-Layer LSTM in Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output size:  torch.Size([5, 1, 4])\n",
      "hn size:  torch.Size([2, 1, 4])\n",
      "cn size:  torch.Size([2, 1, 4])\n"
     ]
    }
   ],
   "source": [
    "output, (hn, cn) = lstm(input, (h0, c0))\n",
    "\n",
    "print(\"Output size: \", output.size())\n",
    "print(\"hn size: \", hn.size())\n",
    "print(\"cn size: \", cn.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **output (seq_len, batch, hidden_size * num_directions):** tensor containing the output features (h_t) \n",
    "from the LAST LAYER of the RNN, for each t (so total of seq_len). Basically, it is all of the hidden states \n",
    "throughout the sequence.\n",
    "- **hn (num_layers * num_directions, batch, hidden_size):** tensor containing the hidden state for t=seq_len.\n",
    "- **cn (num_layers * num_directions, batch, hidden_size):** tensor containing the cell state for t=seq_len."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing the last slice of \"output\" with \"hidden state\" from the last layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The last slice of the output:  \n",
      "-0.1165 -0.1405  0.0563  0.1008\n",
      "[torch.FloatTensor of size 1x4]\n",
      "\n",
      "Hidden state from last layer:  \n",
      "-0.1165 -0.1405  0.0563  0.1008\n",
      "[torch.FloatTensor of size 1x4]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"The last slice of the output: \", output.data[4])\n",
    "print(\"Hidden state from last layer: \", hn.data[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the last slice of \"output\" with \"hidden state from last layer i.e.,hn[num_layers-1] below, they are the same. The reason for this is that: \"output\" will give one access to all hidden states (last hidden layers) in the sequence. While \"hn\" will allow one to continue the sequence and backpropagate, by passing it as an argument  to the lstm at a later time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing the last slice of \"output\" with \"hidden state from second last layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The last slice of the output:  \n",
      "-0.1165 -0.1405  0.0563  0.1008\n",
      "[torch.FloatTensor of size 1x4]\n",
      "\n",
      "Hidden state from second last layer:  \n",
      " 0.1357  0.4084  0.3309  0.0066\n",
      "[torch.FloatTensor of size 1x4]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"The last slice of the output: \", output.data[4])\n",
    "print(\"Hidden state from second last layer: \", hn.data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the last slice of \"output\" with \"hidden state from second last layer i.e.,hn[num_layers-2] below, they are the different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 6])\n",
      "torch.Size([16, 4])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 4])\n",
      "torch.Size([16, 4])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "for param in lstm.parameters():\n",
    "    print(param.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hidden Layer 1\n",
    "\n",
    "torch.Size([16, 6]) \n",
    "weight_ih_l[k] => the learnable input-hidden weights of the 1st layer \n",
    "                        (W_ii|W_if|W_ig|W_io), of shape (4*hidden_size (4) x input_size (6))\n",
    "\n",
    "torch.Size([16, 4])\n",
    "weight_hh_l[k] => the learnable hidden-hidden weights of the 1st layer \n",
    "                        (W_hi|W_hf|W_hg|W_ho), of shape (4*hidden_size (4) x hidden_size (4))\n",
    "\n",
    "torch.Size([16]) \n",
    "bias_ih_l[k] => the learnable input-hidden bias of the 1st layer \n",
    "                        (b_ii|b_if|b_ig|b_io), of shape (4*hidden_size)\n",
    "                        \n",
    "torch.Size([16])\n",
    "bias_hh_l[k] => the learnable hidden-hidden bias of the 1st layer \n",
    "                        (b_hi|b_hf|b_hg|b_ho), of shape (4*hidden_size)\n",
    "\n",
    "### Hidden Layer 2\n",
    "torch.Size([16, 4])\n",
    "torch.Size([16, 4])\n",
    "torch.Size([16])\n",
    "torch.Size([16])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
