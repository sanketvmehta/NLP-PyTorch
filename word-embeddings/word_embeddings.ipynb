{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: sanketvmehta\n",
    "# Base code from: http://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f0782012360>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Embedding module contains \"weight\" as its attribute\n",
    "weight (Tensor): the learnable weights of the module of shape (num_embeddings, embedding_dim)\n",
    "\n",
    "Have a look at the following documentation for more info\n",
    "http://pytorch.org/docs/master/_modules/torch/nn/modules/sparse.html#Embedding\n",
    "'''\n",
    "\n",
    "# an Embedding module containing 20 tensors of size 5\n",
    "embedding = nn.Embedding(20, 5)\n",
    "\n",
    "# a batch of 2 samples of 4 indices each\n",
    "example_lookup_tensor = torch.LongTensor([[1,2,4,5],[4,3,2,9]])\n",
    "input_1 = Variable(example_lookup_tensor)\n",
    "# embedding(input_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example with padding_idx\n",
    "embedding = nn.Embedding(10, 3, padding_idx=5)\n",
    "input_2 = Variable(torch.LongTensor([[0,2,0,5]]))\n",
    "# print(embedding(input_2))\n",
    "\n",
    "# print(Parameter(torch.Tensor(10, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "# Initializing embeddings with pre-trained weights\n",
    "emb = nn.Embedding(10, 2) # construct embedding with desired parameters\n",
    "\n",
    "preloaded_weights = torch.randn(10, 2) # Initialize randomly\n",
    "emb.weight = nn.Parameter(preloaded_weights)\n",
    "# print(emb.weight)\n",
    "\n",
    "# More optimized version which frees space of initial assignment and re-assigns one with new preloaded_weights\n",
    "emb1 = nn.Embedding(10, 3)\n",
    "del emb1.weight\n",
    "preloaded_weights1 = torch.randn(10, 3) # Initialize randomly\n",
    "emb1.weight = nn.Parameter(preloaded_weights1)\n",
    "# print(emb1.weight)\n",
    "\n",
    "'''\n",
    "If one wants to initialize pre-trained weights and freeze them (no further training) \n",
    "while setting \"weight\" attribute to Parameter ()...make its requires_grad = False\n",
    "\n",
    "Have a look at this documentation for more info\n",
    "http://pytorch.org/docs/master/notes/autograd.html#excluding-subgraphs\n",
    "'''\n",
    "\n",
    "emb = nn.Embedding(10, 2) # construct embedding with desired parameters\n",
    "\n",
    "preloaded_weights = torch.randn(10, 2) # Initialize randomly\n",
    "emb.weight = nn.Parameter(preloaded_weights, requires_grad=False)\n",
    "print(emb.weight.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "-0.6021  0.0531 -0.1751 -0.1346 -1.0441\n",
      "[torch.FloatTensor of size 1x5]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "word_to_ix = {\"hello\": 0, \"world\": 1}\n",
    "embeds = nn.Embedding(2, 5)  # 2 words in vocab, 5 dimensional embeddings\n",
    "lookup_tensor = torch.LongTensor([word_to_ix[\"hello\"]])\n",
    "hello_embed = embeds(autograd.Variable(lookup_tensor))\n",
    "print(hello_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.nn.modules.sparse.Embedding"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeds = nn.Embedding(2, 5)\n",
    "type(embeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N-Gram Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['When', 'forty'], 'winters'), (['forty', 'winters'], 'shall'), (['winters', 'shall'], 'besiege')]\n",
      "Variable containing:\n",
      "-2.4774 -0.1273  0.2934 -0.1200 -0.2976 -0.4123 -1.2042 -0.6512  0.5797  0.6054\n",
      "[torch.FloatTensor of size 1x10]\n",
      "\n",
      "<class 'generator'>\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "[\n",
      " 519.3201\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 517.0247\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 514.7433\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 512.4750\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 510.2187\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 507.9735\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 505.7385\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 503.5125\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 501.2951\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 499.0882\n",
      "[torch.FloatTensor of size 1]\n",
      "]\n",
      "Variable containing:\n",
      "-2.4774 -0.1273  0.2934 -0.1200 -0.2976 -0.4123 -1.2042 -0.6512  0.5797  0.6054\n",
      "[torch.FloatTensor of size 1x10]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "CONTEXT_SIZE = 2\n",
    "EMBEDDING_DIM = 10\n",
    "# We will use Shakespeare Sonnet 2\n",
    "test_sentence = \"\"\"When forty winters shall besiege thy brow,\n",
    "And dig deep trenches in thy beauty's field,\n",
    "Thy youth's proud livery so gazed on now,\n",
    "Will be a totter'd weed of small worth held:\n",
    "Then being asked, where all thy beauty lies,\n",
    "Where all the treasure of thy lusty days;\n",
    "To say, within thine own deep sunken eyes,\n",
    "Were an all-eating shame, and thriftless praise.\n",
    "How much more praise deserv'd thy beauty's use,\n",
    "If thou couldst answer 'This fair child of mine\n",
    "Shall sum my count, and make my old excuse,'\n",
    "Proving his beauty by succession thine!\n",
    "This were to be new made when thou art old,\n",
    "And see thy blood warm when thou feel'st it cold.\"\"\".split()\n",
    "\n",
    "# we should tokenize the input, but we will ignore that for now\n",
    "# build a list of tuples.  Each tuple is ([ word_i-2, word_i-1 ], target word)\n",
    "\n",
    "trigrams = [([test_sentence[i], test_sentence[i + 1]], test_sentence[i + 2])\n",
    "            for i in range(len(test_sentence) - 2)]\n",
    "# print the first 3, just so you can see what they look like\n",
    "print(trigrams[:3])\n",
    "\n",
    "vocab = set(test_sentence)\n",
    "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
    "\n",
    "\n",
    "class NGramLanguageModeler(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
    "        \n",
    "#         Your module will always sub-class nn.Module and so you call \n",
    "#         super().__init__() to leverage various functionalities provided by base class nn.Module\n",
    "        super(NGramLanguageModeler, self).__init__()\n",
    "    \n",
    "        '''    \n",
    "        Define the parameters that you will need.  In this case, we need A and b, \n",
    "        the parameters of the embeddings (embedding.weights) and\n",
    "        Torch defines nn.Linear(), which provides the affine map.\n",
    "        '''\n",
    "\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "#         self.embeddings.weight.requires_grad = False\n",
    "        \n",
    "        \n",
    "#         NOTE! The non-linearity log_softmax does not have parameters! So we don't need to worry about that here\n",
    "        \n",
    "        \n",
    "#         Input is context_size (n-1) previous words and each one is of embedding_dim dimension\n",
    "#         Further size of hidden layer in this example is 128\n",
    "        self.linear1 = nn.Linear(context_size * embedding_dim, 128)\n",
    "    \n",
    "#         Output layer is size of our vocabulary\n",
    "        self.linear2 = nn.Linear(128, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "#         inputs is list and so we get tensor of size length of list * embedding_dimension\n",
    "#         in one-dimensional vector so we use view to reShape\n",
    "        embeds = self.embeddings(inputs).view((1, -1))\n",
    "    \n",
    "#         We use relu activation function in first hidden layer and note that we don't have any parameters for non-linearities\n",
    "        out = F.relu(self.linear1(embeds))\n",
    "    \n",
    "        out = self.linear2(out)\n",
    "        \n",
    "#         In last layer we use log_softmax non-linearity\n",
    "        log_probs = F.log_softmax(out)\n",
    "        return log_probs\n",
    "\n",
    "\n",
    "losses = []\n",
    "# Negative log-likelihood loss\n",
    "loss_function = nn.NLLLoss()\n",
    "model = NGramLanguageModeler(len(vocab), EMBEDDING_DIM, CONTEXT_SIZE)\n",
    "\n",
    "print(model.embeddings(autograd.Variable(torch.LongTensor([1]))))\n",
    "\n",
    "# We want to set weight for our embedding to be non-trainable or freeze them\n",
    "model.embeddings.weight.requires_grad = False\n",
    "\n",
    "print(type(model.parameters()))\n",
    "\n",
    "for param in model.parameters():\n",
    "    print(param.requires_grad)\n",
    "\n",
    "#     optimizer requires that all parameters have \"requires_grad\" to be true\n",
    "#     One option is to filter model.parameters() and only pass those which have \"requires_grad\" as true \n",
    "filtered_model_parameters = filter(lambda x: x.requires_grad, model.parameters())\n",
    "\n",
    "optimizer = optim.SGD(filtered_model_parameters, lr=0.001)\n",
    "\n",
    "for epoch in range(10):\n",
    "    total_loss = torch.Tensor([0])\n",
    "    \n",
    "#     trigrams is list of tuples (context, target)\n",
    "    for context, target in trigrams:\n",
    "\n",
    "        # Step 1. Prepare the inputs to be passed to the model (i.e, turn the words\n",
    "        # into integer indices and wrap them in variables)\n",
    "        context_idxs = [word_to_ix[w] for w in context]\n",
    "        context_var = autograd.Variable(torch.LongTensor(context_idxs))\n",
    "\n",
    "        # Step 2. Recall that torch *accumulates* gradients. Before passing in a\n",
    "        # new instance, you need to zero out the gradients from the old\n",
    "        # instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Step 3. Run the forward pass, getting log probabilities over next\n",
    "        # words\n",
    "        log_probs = model(context_var)\n",
    "\n",
    "        # Step 4. Compute your loss function. (Again, Torch wants the target\n",
    "        # word wrapped in a variable)\n",
    "        loss = loss_function(log_probs, autograd.Variable(\n",
    "            torch.LongTensor([word_to_ix[target]])))\n",
    "\n",
    "        # Step 5. Do the backward pass and update the gradient\n",
    "        loss.backward()\n",
    "        \n",
    "#         Update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.data\n",
    "    losses.append(total_loss)\n",
    "print(losses)  # The loss decreased every iteration over the training data!\n",
    "print(model.embeddings(autograd.Variable(torch.LongTensor([1]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
